{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab-12-6-seq-to-seq-with-attention-keras-eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple neural machine translation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "### matplotlib 한글 폰트 설정 #############################\n",
    "from matplotlib import font_manager, rc\n",
    "## for window #####\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "## for mac #####\n",
    "#rc('font', family='AppleGothic') #for mac\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for sources\n",
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
    "\n",
    "pprint(source2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1,\n",
      " '<eos>': 2,\n",
      " '<pad>': 0,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for targets\n",
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
    "\n",
    "pprint(target2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        # preprocessing for source (encoder)\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':\n",
    "        # preprocessing for target (decoder)\n",
    "        # input\n",
    "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
    "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        # output\n",
    "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
    "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] [[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for source\n",
    "s_max_len = 10\n",
    "s_len, s_input = preprocess(sequences = sources,\n",
    "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
    "print(s_len, s_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 6, 6] [[ 1  4  7  3  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  6 10  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  5 11 13  2  0  0  0  0  0  0]\n",
      " [ 1 12  6  9  8  2  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
      " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
      " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
      " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for target\n",
    "t_max_len = 12\n",
    "t_len, t_input, t_output = preprocess(sequences = targets,\n",
    "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
    "print(t_len, t_input, t_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "learning_rate = .005\n",
    "total_step = epochs / batch_size\n",
    "buffer_size = 100\n",
    "n_batch = buffer_size//batch_size\n",
    "embedding_dim = 32\n",
    "units = 128\n",
    "\n",
    "# input\n",
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size = buffer_size)\n",
    "data = data.batch(batch_size = batch_size)\n",
    "# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "#         print(\"state: {}\".format(state.shape))\n",
    "#         print(\"output: {}\".format(state.shape))\n",
    "              \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # * `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "                \n",
    "        #* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # * `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        # * `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # * `merged vector = concat(embedding output, context vector)`\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
    "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "#     print(\"real: {}\".format(real))\n",
    "#     print(\"pred: {}\".format(pred))\n",
    "#     print(\"mask: {}\".format(mask))\n",
    "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# creating optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# creating check point (Object-based saving)\n",
    "checkpoint_dir = './data_out/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)\n",
    "\n",
    "# create writer for tensorboard\n",
    "summary_writer = tf.summary.create_file_writer(logdir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.0396 Batch Loss 0.9895\n",
      "Epoch 10 Loss 0.0376 Batch Loss 0.9410\n",
      "Epoch 20 Loss 0.0344 Batch Loss 0.8605\n",
      "Epoch 30 Loss 0.0328 Batch Loss 0.8193\n",
      "Epoch 40 Loss 0.0301 Batch Loss 0.7530\n",
      "Epoch 50 Loss 0.0232 Batch Loss 0.5803\n",
      "Epoch 60 Loss 0.0168 Batch Loss 0.4188\n",
      "Epoch 70 Loss 0.0111 Batch Loss 0.2769\n",
      "Epoch 80 Loss 0.0068 Batch Loss 0.1695\n",
      "Epoch 90 Loss 0.0038 Batch Loss 0.0958\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n",
    "            \n",
    "            #Teacher Forcing: feeding the target as the next input\n",
    "            for t in range(1, t_input.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(t_input[:, t], predictions)\n",
    "            \n",
    "                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n",
    "                \n",
    "        batch_loss = (loss / int(t_input.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradient = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradient, variables))\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        #save model every 10 epoch\n",
    "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
    "                                            total_loss / n_batch,\n",
    "                                            batch_loss.numpy()))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "#     sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += idx2target[predicted_id] + ' '\n",
    "\n",
    "        if idx2target.get(predicted_id) == '<eos>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "# result, sentence, attention_plot = evaluate(sentence, encoder, decoder, source2idx, target2idx,\n",
    "#                                             s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1f9bf2c9130>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#restore checkpoint\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I feel hungry\n",
      "Predicted translation: 나는 배가 고프다 <eos> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = 'I feel hungry'\n",
    "# sentence = 'tensorflow is a framework for deep learning'\n",
    "\n",
    "translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
