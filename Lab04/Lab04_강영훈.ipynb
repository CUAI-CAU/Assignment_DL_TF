{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCPKMtKQqNnP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P2wN9S9qR9s"
      },
      "source": [
        "tf.random.set_seed(0)  # for reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4u6nzPoqU61"
      },
      "source": [
        "x1_data = [1, 0, 3, 0, 5]\n",
        "x2_data = [0, 2, 0, 4, 0]\n",
        "y_data  = [1, 2, 3, 4, 5]\n",
        "\n",
        "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
        "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
        "b  = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
        "\n",
        "learning_rate = tf.Variable(0.001)\n",
        "\n",
        "for i in range(1000+1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
        "    W1.assign_sub(learning_rate * W1_grad)\n",
        "    W2.assign_sub(learning_rate * W2_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
        "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8izGO_NqWm7"
      },
      "source": [
        "x_data = [\n",
        "    [1., 0., 3., 0., 5.],\n",
        "    [0., 2., 0., 4., 0.]\n",
        "]\n",
        "y_data  = [1, 2, 3, 4, 5]\n",
        "\n",
        "W = tf.Variable(tf.random.uniform((1, 2), -1.0, 1.0))\n",
        "b = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n",
        "\n",
        "learning_rate = tf.Variable(0.001)\n",
        "\n",
        "for i in range(1000+1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "        W.assign_sub(learning_rate * W_grad)\n",
        "        b.assign_sub(learning_rate * b_grad)\n",
        "    \n",
        "    if i % 50 == 0:\n",
        "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
        "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bl-cyfmqYqh"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 앞의 코드에서 bias(b)를 행렬에 추가\n",
        "x_data = [\n",
        "    [1., 1., 1., 1., 1.], # bias(b)\n",
        "    [1., 0., 3., 0., 5.], \n",
        "    [0., 2., 0., 4., 0.]\n",
        "]\n",
        "y_data  = [1, 2, 3, 4, 5]\n",
        "\n",
        "W = tf.Variable(tf.random.uniform((1, 3), -1.0, 1.0)) # [1, 3]으로 변경하고, b 삭제\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "for i in range(1000+1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = tf.matmul(W, x_data) # b가 없다\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "    grads = tape.gradient(cost, [W])\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))\n",
        "    if i % 50 == 0:\n",
        "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(\n",
        "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIdo04j4qgr6"
      },
      "source": [
        "# Multi-variable linear regression (1)\n",
        "\n",
        "X = tf.constant([[1., 2.], \n",
        "                 [3., 4.]])\n",
        "y = tf.constant([[1.5], [3.5]])\n",
        "\n",
        "W = tf.Variable(tf.random.normal((2, 1)))\n",
        "b = tf.Variable(tf.random.normal((1,)))\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "n_epoch = 1000+1\n",
        "print(\"epoch | cost\")\n",
        "for i in range(n_epoch):\n",
        "    # Use tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = tf.matmul(X, W) + b\n",
        "        cost = tf.reduce_mean(tf.square(y_pred - y))\n",
        "\n",
        "    # calculates the gradients of the loss\n",
        "    grads = tape.gradient(cost, [W, b])\n",
        "    \n",
        "    # updates parameters (W and b)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
        "    if i % 50 == 0:\n",
        "        print(\"{:5} | {:10.6f}\".format(i, cost.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAPVdllGqiJs"
      },
      "source": [
        "tf.random.set_seed(0)  # for reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMx_5-IDqj_q"
      },
      "source": [
        "# data and label\n",
        "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
        "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
        "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
        "Y  = [152., 185., 180., 196., 142.]\n",
        "\n",
        "# weights\n",
        "w1 = tf.Variable(10.)\n",
        "w2 = tf.Variable(10.)\n",
        "w3 = tf.Variable(10.)\n",
        "b  = tf.Variable(10.)\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "for i in range(1000+1):\n",
        "    # tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "    # calculates the gradients of the cost\n",
        "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
        "    \n",
        "    # update w1,w2,w3 and b\n",
        "    w1.assign_sub(learning_rate * w1_grad)\n",
        "    w2.assign_sub(learning_rate * w2_grad)\n",
        "    w3.assign_sub(learning_rate * w3_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY21VJgGqlWV"
      },
      "source": [
        "# data and label\n",
        "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
        "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
        "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
        "Y  = [152., 185., 180., 196., 142.]\n",
        "\n",
        "# random weights\n",
        "w1 = tf.Variable(tf.random.normal((1,)))\n",
        "w2 = tf.Variable(tf.random.normal((1,)))\n",
        "w3 = tf.Variable(tf.random.normal((1,)))\n",
        "b  = tf.Variable(tf.random.normal((1,)))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "for i in range(1000+1):\n",
        "    # tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
        "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "    # calculates the gradients of the cost\n",
        "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
        "    \n",
        "    # update w1,w2,w3 and b\n",
        "    w1.assign_sub(learning_rate * w1_grad)\n",
        "    w2.assign_sub(learning_rate * w2_grad)\n",
        "    w3.assign_sub(learning_rate * w3_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP1kk1gGqnIc"
      },
      "source": [
        "data = np.array([\n",
        "    # X1,   X2,    X3,   y\n",
        "    [ 73.,  80.,  75., 152. ],\n",
        "    [ 93.,  88.,  93., 185. ],\n",
        "    [ 89.,  91.,  90., 180. ],\n",
        "    [ 96.,  98., 100., 196. ],\n",
        "    [ 73.,  66.,  70., 142. ]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# slice data\n",
        "X = data[:, :-1]\n",
        "y = data[:, [-1]]\n",
        "\n",
        "W = tf.Variable(tf.random.normal((3, 1)))\n",
        "b = tf.Variable(tf.random.normal((1,)))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "# hypothesis, prediction function\n",
        "def predict(X):\n",
        "    return tf.matmul(X, W) + b\n",
        "\n",
        "print(\"epoch | cost\")\n",
        "\n",
        "n_epochs = 2000\n",
        "for i in range(n_epochs+1):\n",
        "    # tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
        "\n",
        "    # calculates the gradients of the loss\n",
        "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "    # updates parameters (W and b)\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPT-U8waqoys"
      },
      "source": [
        "W.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuGZBw4NqqCY"
      },
      "source": [
        "b.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAVgQy6Qqq0T"
      },
      "source": [
        "tf.matmul(X, W) + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlFtzpfKqsPI"
      },
      "source": [
        "Y # labels, 실제값"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56DNu2wqtOf"
      },
      "source": [
        "predict(X).numpy() # prediction, 예측값"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXovzixequEU"
      },
      "source": [
        "# 새로운 데이터에 대한 예측\n",
        "\n",
        "predict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-UtYwspqvXk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}