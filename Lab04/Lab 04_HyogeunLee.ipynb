{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3f743c",
   "metadata": {},
   "source": [
    "# Lab 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c4407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74d7d0",
   "metadata": {},
   "source": [
    "## Hypothesis using matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c83122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data and label\n",
    "x1=[73.,93.,89.,96.,73.]\n",
    "x2=[80.,88.,91.,98.,66.]\n",
    "x3=[75.,93.,90.,100.,70.]\n",
    "Y=[152.,185.,180.,196.,142.]\n",
    "\n",
    "# weights\n",
    "w1=tf.Variable(10.)\n",
    "w2=tf.Variable(10.)\n",
    "w3=tf.Variable(10.)\n",
    "b=tf.Variable(10.)\n",
    "\n",
    "hypothesis=w1*x1+w2*x2+w3*x3+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b730ce6",
   "metadata": {},
   "source": [
    "- x의 개수가 3개이므로 그에 대응되는 weigts도 세 개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e9d9f",
   "metadata": {},
   "source": [
    "## Gradient descent in Multi variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69fe03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=tf.Variable(tf.random.normal([1]))\n",
    "w2=tf.Variable(tf.random.normal([1]))\n",
    "w3=tf.Variable(tf.random.normal([1]))\n",
    "b=tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate=0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ca8dd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0| 191937.3594\n",
      "   50|   2156.4238\n",
      "  100|     50.5431\n",
      "  150|     27.1130\n",
      "  200|     26.7900\n",
      "  250|     26.7236\n",
      "  300|     26.6602\n",
      "  350|     26.5969\n",
      "  400|     26.5339\n",
      "  450|     26.4710\n",
      "  500|     26.4084\n",
      "  550|     26.3458\n",
      "  600|     26.2835\n",
      "  650|     26.2212\n",
      "  700|     26.1591\n",
      "  750|     26.0973\n",
      "  800|     26.0357\n",
      "  850|     25.9741\n",
      "  900|     25.9127\n",
      "  950|     25.8516\n",
      " 1000|     25.7905\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000+1):\n",
    "    #tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis=w1*x1+w2*x2+w3*x3+b\n",
    "        cost=tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "        \n",
    "    # calculates the gradients of the cost\n",
    "    w1_grad, w2_grad, w3_grad, b_grad=tape.gradient(cost,[w1,w2,w3,b])\n",
    "    \n",
    "    #update w1,w2,w3 and b\n",
    "    w1.assign_sub(learning_rate*w1_grad)\n",
    "    w2.assign_sub(learning_rate*w2_grad)\n",
    "    w3.assign_sub(learning_rate*w3_grad)\n",
    "    b.assign_sub(learning_rate*b_grad)\n",
    "    \n",
    "    if i%50==0:\n",
    "        print(\"{:5}|{:12.4f}\".format(i,cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17cd7f4",
   "metadata": {},
   "source": [
    "- 각 w가 tf.random.normal([1]) 사용했으므로 실행시킬 때마다 값이 달라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cebc6b",
   "metadata": {},
   "source": [
    "## Gradient using matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb44a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= np.array([\n",
    "    #X1,  X2,  X3,   y\n",
    "    [73., 80., 75., 152. ],\n",
    "    [93., 88., 93., 185. ],\n",
    "    [89., 91., 90., 180. ],\n",
    "    [96., 98.,100., 196. ],\n",
    "    [73., 66., 70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "X=data[:,:-1] #data[행, 열], 행 전체, 마지막 컬럼제외\n",
    "y=data[:,[-1]] # 행 전체, 마지막 컬럼만\n",
    "\n",
    "W=tf.Variable(tf.random.normal([3,1])) #weight의 행 수는 X 행 수와 같아야함\n",
    "b=tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "def predict(X):\n",
    "    return tf.matmul(X,W)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9685f2",
   "metadata": {},
   "source": [
    "- x1,x2,x3 따로 따로 변수를 지정할 필요가 없음!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf67bf7",
   "metadata": {},
   "source": [
    "## All Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a535cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|196475.7812\n",
      "  100|   29.1950\n",
      "  200|    4.9837\n",
      "  300|    4.9561\n",
      "  400|    4.9317\n",
      "  500|    4.9074\n",
      "  600|    4.8832\n",
      "  700|    4.8592\n",
      "  800|    4.8354\n",
      "  900|    4.8116\n",
      " 1000|    4.7879\n",
      " 1100|    4.7644\n",
      " 1200|    4.7411\n",
      " 1300|    4.7178\n",
      " 1400|    4.6947\n",
      " 1500|    4.6716\n",
      " 1600|    4.6487\n",
      " 1700|    4.6260\n",
      " 1800|    4.6034\n",
      " 1900|    4.5808\n",
      " 2000|    4.5584\n"
     ]
    }
   ],
   "source": [
    "data= np.array([\n",
    "    #X1,  X2,  X3,   y\n",
    "    [73., 80., 75., 152. ],\n",
    "    [93., 88., 93., 185. ],\n",
    "    [89., 91., 90., 180. ],\n",
    "    [96., 98.,100., 196. ],\n",
    "    [73., 66., 70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "X=data[:,:-1] \n",
    "y=data[:,[-1]] \n",
    "\n",
    "W=tf.Variable(tf.random.normal([3,1]))\n",
    "b=tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate=0.000001\n",
    "# hypothesis, prediction function\n",
    "def predict(X):\n",
    "    return tf.matmul(X,W)+b\n",
    "\n",
    "n_epochs=2000\n",
    "for i in range(n_epochs+1):\n",
    "    # record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost=tf.reduce_mean((tf.square(predict(X)-y)))\n",
    "    \n",
    "    #calculates the gradients of the loss\n",
    "    W_grad,b_grad=tape.gradient(cost, [W,b])\n",
    "\n",
    "    # updates parameters(W and b)\n",
    "    W.assign_sub(learning_rate*W_grad)\n",
    "    b.assign_sub(learning_rate*b_grad)\n",
    "    \n",
    "    if i %100==0:\n",
    "        print(\"{:5}|{:10.4f}\".format(i,cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31aceef",
   "metadata": {},
   "source": [
    "# With Matrix vs not use matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15f9b781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(1,) dtype=float32, numpy=array([-0.7963815], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with Matrix\n",
    "\n",
    "#initialize W\n",
    "W=tf.Variable(tf.random.normal([3,1]))\n",
    "\n",
    "# hypothesis, prediction function\n",
    "tf.matmul(X,W)+b\n",
    "\n",
    "# updates parameters (W and b)\n",
    "W.assign_sub(learning_rate * W_grad)\n",
    "\n",
    "\n",
    "# not use Matrix\n",
    "\n",
    "# innitialize W\n",
    "w1=tf.Variable(tf.random.normal([1]))\n",
    "w2=tf.Variable(tf.random.normal([1]))\n",
    "w3=tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "# hypothesis, prediction function\n",
    "w1*x1+w2*x2+w3*x3+b\n",
    "\n",
    "#update w1, w2, w3\n",
    "w1.assign_sub(learning_rate*w1_grad)\n",
    "w2.assign_sub(learning_rate*w2_grad)\n",
    "w3.assign_sub(learning_rate*w3_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
